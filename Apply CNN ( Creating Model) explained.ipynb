{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here initially the required libraries are initially imported to the program like **OpenCV (cv2), NumPy , Math , Sys , os .**\n",
    "Then , the **Tensorflow** Module is imported followed by **Keras** from where *Sub-Modules :  Sequential , Dense, Conv2D, MaxPooling2D, Dropout, Flatten* are imported.\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #print(logs.get('acc'))\n",
    "        if(logs.get('acc')!= None and logs.get('acc')>0.99):\n",
    "            print(\"\\nReached 99% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def image_resize(image, height = 45, inter = cv2.INTER_AREA):\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "    if(w>h):\n",
    "    \tr = height / float(h)\n",
    "    \tdim = (int(w * r), height)\n",
    "    else:\n",
    "    \tr = height / float(w)\n",
    "    \tdim = (height, int(h * r))\n",
    "    resized = cv2.resize(image, dim, interpolation = inter)\n",
    "    return resized\n",
    "\n",
    "def data_read(data_path,category='skin',source='Dataset_skin'):\n",
    "\tlabels = []\n",
    "\timages = []\n",
    "\tdata_path = os.path.join(data_path,source)\n",
    "\tfor label in os.listdir(data_path):\n",
    "\t\tlabel_path = os.path.join(data_path,label,category)\n",
    "\t\tfor image in os.listdir(label_path):\n",
    "\t\t\timg = cv2.imread(os.path.join(label_path,image))\n",
    "\t\t\tif(img is not None):\n",
    "\t\t\t\tresized_img = image_resize(img)\n",
    "\t\t\t\timages.append(resized_img)\n",
    "\t\t\t\tlabels.append(label)\n",
    "\treturn labels, images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the **Keras** Module the method **on_epoch_end()** from the class **myCallback(tf.keras.callbacks.Callback)** is overloaded and updated as per the requirements , such that the training of the model automatically stops on reaching a accuracy of 99%\n",
    "\n",
    "The function **image_resize()** is defined which resizes the input image to the model for training as *45*45* image , and returns the resized image as a output . \n",
    "\n",
    "The function **data_read()** reads all the images from the categorical folfer named 'skin' under the directory  names \"Dataset\" and stores the images in the form of array . the collection of the array_of_images is stored under the variable named *images* and labels store the alphabet they are representing .\n",
    "\n",
    "\n",
    "*Example ==>* An image represting 'A ' is stored as :\n",
    "\n",
    "   *Image Data* :    [ [ [ 15 , 15 , 16 ] , [ 22 , 22 , 24 ] , [ 1 , 0 , 1 ] , [ 0 , 0 . . . . ] \n",
    "   \n",
    "   *Label*      :     A\n",
    "\n",
    "and the same is returned as a Numpy array of Image arrays called *images* and a list of respective denoying alphabet as *labels* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, images = data_read(r'C:\\Users\\ndh60048\\Documents\\ASL translator')\n",
    "data = pd.DataFrame({'Image Data':images,'Label':labels})\n",
    "print('Data Sample')\n",
    "print(data.head(10))\n",
    "\n",
    "\n",
    "label_cats = np.unique(labels)\n",
    "int_encoding = np.arange(len(label_cats))\n",
    "label_to_int = pd.DataFrame({'Label':label_cats,'Encoded':int_encoding})\n",
    "label_to_int.to_csv('label_encoded.csv')\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "labels_arr = np.array([[i] for i in labels]).reshape(len(labels),1)\n",
    "labels_encoded = onehot_encoder.fit_transform(labels_arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initially  , The data_read() is called by passing the path of the Dataset Folder as a argument to the function.\n",
    "and labels are encoded as integers and stored in a .csv file\n",
    "\n",
    "the encoded labels are then passed through the *OneHotEncoder()* , which one_hot_encodes the labels.\n",
    "\n",
    "\n",
    "Example :\n",
    " Label - **D**\n",
    " \n",
    " after OneHotEncoding becomes : \n",
    " **array([0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ] )**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 45, 3)\n",
      "Epoch 1/200\n",
      "11645/11645 [==============================] - 168s 14ms/step - loss: 3.1507 - accuracy: 0.1717\n",
      "Epoch 2/200\n",
      "11645/11645 [==============================] - 160s 14ms/step - loss: 0.9788 - accuracy: 0.6927\n",
      "Epoch 3/200\n",
      "11645/11645 [==============================] - 157s 13ms/step - loss: 0.3866 - accuracy: 0.8769\n",
      "Epoch 4/200\n",
      "11645/11645 [==============================] - 154s 13ms/step - loss: 0.2226 - accuracy: 0.9299\n",
      "Epoch 5/200\n",
      "11645/11645 [==============================] - 148s 13ms/step - loss: 0.1569 - accuracy: 0.9509\n",
      "Epoch 6/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.1233 - accuracy: 0.9597\n",
      "Epoch 7/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0996 - accuracy: 0.9703\n",
      "Epoch 8/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0771 - accuracy: 0.9755\n",
      "Epoch 9/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0698 - accuracy: 0.9785\n",
      "Epoch 10/200\n",
      "11645/11645 [==============================] - 144s 12ms/step - loss: 0.0560 - accuracy: 0.9830\n",
      "Epoch 11/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0558 - accuracy: 0.9813\n",
      "Epoch 12/200\n",
      "11645/11645 [==============================] - 147s 13ms/step - loss: 0.0606 - accuracy: 0.9807\n",
      "Epoch 13/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0450 - accuracy: 0.9860\n",
      "Epoch 14/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0402 - accuracy: 0.9869\n",
      "Epoch 15/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0382 - accuracy: 0.9875\n",
      "Epoch 16/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0403 - accuracy: 0.9877\n",
      "Epoch 17/200\n",
      "11645/11645 [==============================] - 144s 12ms/step - loss: 0.0235 - accuracy: 0.9918\n",
      "Epoch 18/200\n",
      "11645/11645 [==============================] - 146s 13ms/step - loss: 0.0277 - accuracy: 0.9910\n",
      "Epoch 19/200\n",
      "11645/11645 [==============================] - 145s 12ms/step - loss: 0.0357 - accuracy: 0.9891\n",
      "Epoch 20/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0265 - accuracy: 0.9912\n",
      "Epoch 21/200\n",
      "11645/11645 [==============================] - 144s 12ms/step - loss: 0.0285 - accuracy: 0.9916\n",
      "Epoch 22/200\n",
      "11645/11645 [==============================] - 144s 12ms/step - loss: 0.0266 - accuracy: 0.9913\n",
      "Epoch 23/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0284 - accuracy: 0.9908\n",
      "Epoch 24/200\n",
      "11645/11645 [==============================] - 145s 12ms/step - loss: 0.0212 - accuracy: 0.9938\n",
      "Epoch 25/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0240 - accuracy: 0.9924\n",
      "Epoch 26/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0239 - accuracy: 0.9926\n",
      "Epoch 27/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0269 - accuracy: 0.9912\n",
      "Epoch 28/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0285 - accuracy: 0.9924\n",
      "Epoch 29/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0286 - accuracy: 0.9894\n",
      "Epoch 30/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0317 - accuracy: 0.9914\n",
      "Epoch 31/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0235 - accuracy: 0.9916\n",
      "Epoch 32/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0250 - accuracy: 0.9926\n",
      "Epoch 33/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0239 - accuracy: 0.9920\n",
      "Epoch 34/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0255 - accuracy: 0.9918\n",
      "Epoch 35/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0128 - accuracy: 0.9960\n",
      "Epoch 36/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0180 - accuracy: 0.9931\n",
      "Epoch 37/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0142 - accuracy: 0.9950\n",
      "Epoch 38/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0317 - accuracy: 0.9914\n",
      "Epoch 39/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0216 - accuracy: 0.9936\n",
      "Epoch 40/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0295 - accuracy: 0.9915\n",
      "Epoch 41/200\n",
      "11645/11645 [==============================] - 147s 13ms/step - loss: 0.0165 - accuracy: 0.9965\n",
      "Epoch 42/200\n",
      "11645/11645 [==============================] - 166s 14ms/step - loss: 0.0167 - accuracy: 0.9945\n",
      "Epoch 43/200\n",
      "11645/11645 [==============================] - 157s 13ms/step - loss: 0.0132 - accuracy: 0.9956\n",
      "Epoch 44/200\n",
      "11645/11645 [==============================] - 160s 14ms/step - loss: 0.0206 - accuracy: 0.9937\n",
      "Epoch 45/200\n",
      "11645/11645 [==============================] - 165s 14ms/step - loss: 0.0134 - accuracy: 0.9961\n",
      "Epoch 46/200\n",
      "11645/11645 [==============================] - 167s 14ms/step - loss: 0.0210 - accuracy: 0.9940\n",
      "Epoch 47/200\n",
      "11645/11645 [==============================] - 179s 15ms/step - loss: 0.0126 - accuracy: 0.9967\n",
      "Epoch 48/200\n",
      "11645/11645 [==============================] - 179s 15ms/step - loss: 0.0082 - accuracy: 0.9978\n",
      "Epoch 49/200\n",
      "11645/11645 [==============================] - 177s 15ms/step - loss: 0.0140 - accuracy: 0.9965\n",
      "Epoch 50/200\n",
      "11645/11645 [==============================] - 181s 16ms/step - loss: 0.0277 - accuracy: 0.9919\n",
      "Epoch 51/200\n",
      "11645/11645 [==============================] - 194s 17ms/step - loss: 0.0291 - accuracy: 0.9915\n",
      "Epoch 52/200\n",
      "11645/11645 [==============================] - 176s 15ms/step - loss: 0.0138 - accuracy: 0.9954\n",
      "Epoch 53/200\n",
      "11645/11645 [==============================] - 174s 15ms/step - loss: 0.0155 - accuracy: 0.9948\n",
      "Epoch 54/200\n",
      "11645/11645 [==============================] - 189s 16ms/step - loss: 0.0112 - accuracy: 0.9967\n",
      "Epoch 55/200\n",
      "11645/11645 [==============================] - 184s 16ms/step - loss: 0.0193 - accuracy: 0.9940\n",
      "Epoch 56/200\n",
      "11645/11645 [==============================] - 179s 15ms/step - loss: 0.0147 - accuracy: 0.9951\n",
      "Epoch 57/200\n",
      "11645/11645 [==============================] - 185s 16ms/step - loss: 0.0136 - accuracy: 0.9959\n",
      "Epoch 58/200\n",
      "11645/11645 [==============================] - 186s 16ms/step - loss: 0.0210 - accuracy: 0.9942\n",
      "Epoch 59/200\n",
      "11645/11645 [==============================] - 193s 17ms/step - loss: 0.0140 - accuracy: 0.9960\n",
      "Epoch 60/200\n",
      "11645/11645 [==============================] - 175s 15ms/step - loss: 0.0178 - accuracy: 0.9943\n",
      "Epoch 61/200\n",
      "11645/11645 [==============================] - 181s 16ms/step - loss: 0.0135 - accuracy: 0.9956\n",
      "Epoch 62/200\n",
      "11645/11645 [==============================] - 171s 15ms/step - loss: 0.0132 - accuracy: 0.9956\n",
      "Epoch 63/200\n",
      "11645/11645 [==============================] - 178s 15ms/step - loss: 0.0185 - accuracy: 0.9948\n",
      "Epoch 64/200\n",
      "11645/11645 [==============================] - 193s 17ms/step - loss: 0.0214 - accuracy: 0.9937\n",
      "Epoch 65/200\n",
      "11645/11645 [==============================] - 186s 16ms/step - loss: 0.0067 - accuracy: 0.9977\n",
      "Epoch 66/200\n",
      "11645/11645 [==============================] - 192s 16ms/step - loss: 0.0155 - accuracy: 0.9957\n",
      "Epoch 67/200\n",
      "11645/11645 [==============================] - 177s 15ms/step - loss: 0.0302 - accuracy: 0.9925\n",
      "Epoch 68/200\n",
      "11645/11645 [==============================] - 182s 16ms/step - loss: 0.0135 - accuracy: 0.9959\n",
      "Epoch 69/200\n",
      "11645/11645 [==============================] - 185s 16ms/step - loss: 0.0056 - accuracy: 0.9985\n",
      "Epoch 70/200\n",
      "11645/11645 [==============================] - 178s 15ms/step - loss: 0.0044 - accuracy: 0.9986\n",
      "Epoch 71/200\n",
      "11645/11645 [==============================] - 181s 16ms/step - loss: 0.0126 - accuracy: 0.9970\n",
      "Epoch 72/200\n",
      "11645/11645 [==============================] - 181s 16ms/step - loss: 0.0118 - accuracy: 0.9967\n",
      "Epoch 73/200\n",
      "11645/11645 [==============================] - 177s 15ms/step - loss: 0.0137 - accuracy: 0.9960\n",
      "Epoch 74/200\n",
      "11645/11645 [==============================] - 172s 15ms/step - loss: 0.0131 - accuracy: 0.9958\n",
      "Epoch 75/200\n",
      "11645/11645 [==============================] - 171s 15ms/step - loss: 0.0146 - accuracy: 0.9962\n",
      "Epoch 76/200\n",
      "11645/11645 [==============================] - 176s 15ms/step - loss: 0.0149 - accuracy: 0.9961\n",
      "Epoch 77/200\n",
      "11645/11645 [==============================] - 186s 16ms/step - loss: 0.0031 - accuracy: 0.9988\n",
      "Epoch 78/200\n",
      "11645/11645 [==============================] - 174s 15ms/step - loss: 0.0205 - accuracy: 0.9949\n",
      "Epoch 79/200\n",
      "11645/11645 [==============================] - 180s 16ms/step - loss: 0.0136 - accuracy: 0.9960\n",
      "Epoch 80/200\n",
      "11645/11645 [==============================] - 185s 16ms/step - loss: 0.0101 - accuracy: 0.9965\n",
      "Epoch 81/200\n",
      "11645/11645 [==============================] - 160s 14ms/step - loss: 0.0196 - accuracy: 0.9949\n",
      "Epoch 82/200\n",
      "11645/11645 [==============================] - 167s 14ms/step - loss: 0.0149 - accuracy: 0.9959\n",
      "Epoch 83/200\n",
      "11645/11645 [==============================] - 159s 14ms/step - loss: 0.0167 - accuracy: 0.9954\n",
      "Epoch 84/200\n",
      "11645/11645 [==============================] - 166s 14ms/step - loss: 0.0116 - accuracy: 0.9971\n",
      "Epoch 85/200\n",
      "11645/11645 [==============================] - 159s 14ms/step - loss: 0.0139 - accuracy: 0.9963\n",
      "Epoch 86/200\n",
      "11645/11645 [==============================] - 160s 14ms/step - loss: 0.0146 - accuracy: 0.9964\n",
      "Epoch 87/200\n",
      "11645/11645 [==============================] - 158s 14ms/step - loss: 0.0203 - accuracy: 0.9953\n",
      "Epoch 88/200\n",
      "11645/11645 [==============================] - 154s 13ms/step - loss: 0.0077 - accuracy: 0.9975\n",
      "Epoch 89/200\n",
      "11645/11645 [==============================] - 153s 13ms/step - loss: 0.0073 - accuracy: 0.9976\n",
      "Epoch 90/200\n",
      "11645/11645 [==============================] - 151s 13ms/step - loss: 0.0114 - accuracy: 0.9964\n",
      "Epoch 91/200\n",
      "11645/11645 [==============================] - 152s 13ms/step - loss: 0.0153 - accuracy: 0.9960\n",
      "Epoch 92/200\n",
      "11645/11645 [==============================] - 163s 14ms/step - loss: 0.0149 - accuracy: 0.9965\n",
      "Epoch 93/200\n",
      "11645/11645 [==============================] - 157s 14ms/step - loss: 0.0117 - accuracy: 0.9965\n",
      "Epoch 94/200\n",
      "11645/11645 [==============================] - 145s 12ms/step - loss: 0.0194 - accuracy: 0.9958\n",
      "Epoch 95/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0044 - accuracy: 0.9985\n",
      "Epoch 96/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0039 - accuracy: 0.9988\n",
      "Epoch 97/200\n",
      "11645/11645 [==============================] - 145s 12ms/step - loss: 0.0032 - accuracy: 0.9989\n",
      "Epoch 98/200\n",
      "11645/11645 [==============================] - 144s 12ms/step - loss: 0.0077 - accuracy: 0.9975\n",
      "Epoch 99/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0235 - accuracy: 0.9942\n",
      "Epoch 100/200\n",
      "11645/11645 [==============================] - 139s 12ms/step - loss: 0.0307 - accuracy: 0.9923\n",
      "Epoch 101/200\n",
      "11645/11645 [==============================] - 140s 12ms/step - loss: 0.0220 - accuracy: 0.9947\n",
      "Epoch 102/200\n",
      "11645/11645 [==============================] - 140s 12ms/step - loss: 0.0064 - accuracy: 0.9985\n",
      "Epoch 103/200\n",
      "11645/11645 [==============================] - 140s 12ms/step - loss: 0.0099 - accuracy: 0.9969\n",
      "Epoch 104/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0035 - accuracy: 0.9985\n",
      "Epoch 105/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0044 - accuracy: 0.9987\n",
      "Epoch 106/200\n",
      "11645/11645 [==============================] - 140s 12ms/step - loss: 0.0073 - accuracy: 0.9982\n",
      "Epoch 107/200\n",
      "11645/11645 [==============================] - 138s 12ms/step - loss: 0.0057 - accuracy: 0.9980\n",
      "Epoch 108/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0037 - accuracy: 0.9986\n",
      "Epoch 109/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0126 - accuracy: 0.9965\n",
      "Epoch 110/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0334 - accuracy: 0.9916\n",
      "Epoch 111/200\n",
      "11645/11645 [==============================] - 149s 13ms/step - loss: 0.0160 - accuracy: 0.9958\n",
      "Epoch 112/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0093 - accuracy: 0.9977\n",
      "Epoch 113/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0058 - accuracy: 0.9983\n",
      "Epoch 114/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0082 - accuracy: 0.9976\n",
      "Epoch 115/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0054 - accuracy: 0.9987\n",
      "Epoch 116/200\n",
      "11645/11645 [==============================] - 141s 12ms/step - loss: 0.0067 - accuracy: 0.9978\n",
      "Epoch 117/200\n",
      "11645/11645 [==============================] - 150s 13ms/step - loss: 0.0170 - accuracy: 0.9956\n",
      "Epoch 118/200\n",
      "11645/11645 [==============================] - 165s 14ms/step - loss: 0.0099 - accuracy: 0.9973\n",
      "Epoch 119/200\n",
      "11645/11645 [==============================] - 158s 14ms/step - loss: 0.0106 - accuracy: 0.9970\n",
      "Epoch 120/200\n",
      "11645/11645 [==============================] - 161s 14ms/step - loss: 0.0101 - accuracy: 0.9978\n",
      "Epoch 121/200\n",
      "11645/11645 [==============================] - 158s 14ms/step - loss: 0.0158 - accuracy: 0.9968\n",
      "Epoch 122/200\n",
      "11645/11645 [==============================] - 160s 14ms/step - loss: 0.0046 - accuracy: 0.9982\n",
      "Epoch 123/200\n",
      "11645/11645 [==============================] - 160s 14ms/step - loss: 0.0120 - accuracy: 0.9961\n",
      "Epoch 124/200\n",
      "11645/11645 [==============================] - 158s 14ms/step - loss: 0.0062 - accuracy: 0.9988\n",
      "Epoch 125/200\n",
      "11645/11645 [==============================] - 160s 14ms/step - loss: 0.0115 - accuracy: 0.9961\n",
      "Epoch 126/200\n",
      "11645/11645 [==============================] - 157s 14ms/step - loss: 0.0139 - accuracy: 0.9967\n",
      "Epoch 127/200\n",
      "11645/11645 [==============================] - 172s 15ms/step - loss: 0.0108 - accuracy: 0.9967\n",
      "Epoch 128/200\n",
      "11645/11645 [==============================] - 164s 14ms/step - loss: 0.0106 - accuracy: 0.9979\n",
      "Epoch 129/200\n",
      "11645/11645 [==============================] - 150s 13ms/step - loss: 0.0147 - accuracy: 0.9959\n",
      "Epoch 130/200\n",
      "11645/11645 [==============================] - 115s 10ms/step - loss: 0.0083 - accuracy: 0.9979\n",
      "Epoch 131/200\n",
      "11645/11645 [==============================] - 123s 11ms/step - loss: 0.0118 - accuracy: 0.9973\n",
      "Epoch 132/200\n",
      "11645/11645 [==============================] - 114s 10ms/step - loss: 0.0106 - accuracy: 0.9966\n",
      "Epoch 133/200\n",
      "11645/11645 [==============================] - 118s 10ms/step - loss: 0.0124 - accuracy: 0.9975\n",
      "Epoch 134/200\n",
      "11645/11645 [==============================] - 126s 11ms/step - loss: 0.0057 - accuracy: 0.9981\n",
      "Epoch 135/200\n",
      "11645/11645 [==============================] - 120s 10ms/step - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 136/200\n",
      "11645/11645 [==============================] - 33136s 3s/step - loss: 0.0062 - accuracy: 0.9983\n",
      "Epoch 137/200\n",
      "11645/11645 [==============================] - 134s 12ms/step - loss: 0.0148 - accuracy: 0.9970\n",
      "Epoch 138/200\n",
      "11645/11645 [==============================] - 139s 12ms/step - loss: 0.0079 - accuracy: 0.9974\n",
      "Epoch 139/200\n",
      "11645/11645 [==============================] - 142s 12ms/step - loss: 0.0053 - accuracy: 0.9982\n",
      "Epoch 140/200\n",
      "11645/11645 [==============================] - 133s 11ms/step - loss: 0.0097 - accuracy: 0.9978\n",
      "Epoch 141/200\n",
      "11645/11645 [==============================] - 196s 17ms/step - loss: 0.0021 - accuracy: 0.9994\n",
      "Epoch 142/200\n",
      "11645/11645 [==============================] - 184s 16ms/step - loss: 0.0134 - accuracy: 0.9976\n",
      "Epoch 143/200\n",
      "11645/11645 [==============================] - 195s 17ms/step - loss: 0.0165 - accuracy: 0.9954\n",
      "Epoch 144/200\n",
      "11645/11645 [==============================] - 178s 15ms/step - loss: 0.0021 - accuracy: 0.9991\n",
      "Epoch 145/200\n",
      "11645/11645 [==============================] - 139s 12ms/step - loss: 0.0195 - accuracy: 0.9951\n",
      "Epoch 146/200\n",
      "11645/11645 [==============================] - 129s 11ms/step - loss: 0.0158 - accuracy: 0.9970\n",
      "Epoch 147/200\n",
      "11645/11645 [==============================] - 127s 11ms/step - loss: 0.0146 - accuracy: 0.9979\n",
      "Epoch 148/200\n",
      "11645/11645 [==============================] - 131s 11ms/step - loss: 0.0092 - accuracy: 0.9974\n",
      "Epoch 149/200\n",
      "11645/11645 [==============================] - 130s 11ms/step - loss: 0.0053 - accuracy: 0.9988\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11645/11645 [==============================] - 130s 11ms/step - loss: 0.0018 - accuracy: 0.9994\n",
      "Epoch 151/200\n",
      "11645/11645 [==============================] - 133s 11ms/step - loss: 0.0076 - accuracy: 0.9978\n",
      "Epoch 152/200\n",
      "11645/11645 [==============================] - 131s 11ms/step - loss: 0.0090 - accuracy: 0.9978\n",
      "Epoch 153/200\n",
      "11645/11645 [==============================] - 131s 11ms/step - loss: 0.0069 - accuracy: 0.9983\n",
      "Epoch 154/200\n",
      "11645/11645 [==============================] - 132s 11ms/step - loss: 0.0086 - accuracy: 0.9979\n",
      "Epoch 155/200\n",
      "11645/11645 [==============================] - 129s 11ms/step - loss: 0.0072 - accuracy: 0.9982\n",
      "Epoch 156/200\n",
      "11645/11645 [==============================] - 129s 11ms/step - loss: 0.0064 - accuracy: 0.9984\n",
      "Epoch 157/200\n",
      "11645/11645 [==============================] - 134s 12ms/step - loss: 0.0116 - accuracy: 0.9972\n",
      "Epoch 158/200\n",
      "11645/11645 [==============================] - 138s 12ms/step - loss: 0.0034 - accuracy: 0.9987\n",
      "Epoch 159/200\n",
      "11645/11645 [==============================] - 125s 11ms/step - loss: 0.0033 - accuracy: 0.9989\n",
      "Epoch 160/200\n",
      "11645/11645 [==============================] - 134s 12ms/step - loss: 0.0058 - accuracy: 0.9986\n",
      "Epoch 161/200\n",
      "11645/11645 [==============================] - 137s 12ms/step - loss: 0.0351 - accuracy: 0.9932\n",
      "Epoch 162/200\n",
      "11645/11645 [==============================] - 134s 11ms/step - loss: 0.0092 - accuracy: 0.9975\n",
      "Epoch 163/200\n",
      "11645/11645 [==============================] - 133s 11ms/step - loss: 0.0090 - accuracy: 0.9976\n",
      "Epoch 164/200\n",
      "11645/11645 [==============================] - 129s 11ms/step - loss: 0.0066 - accuracy: 0.9986\n",
      "Epoch 165/200\n",
      "11645/11645 [==============================] - 133s 11ms/step - loss: 0.0073 - accuracy: 0.9981\n",
      "Epoch 166/200\n",
      "11645/11645 [==============================] - 139s 12ms/step - loss: 0.0058 - accuracy: 0.9984\n",
      "Epoch 167/200\n",
      "11645/11645 [==============================] - 143s 12ms/step - loss: 0.0078 - accuracy: 0.9979\n",
      "Epoch 168/200\n",
      "11645/11645 [==============================] - 140s 12ms/step - loss: 0.0079 - accuracy: 0.9985\n",
      "Epoch 169/200\n",
      "11645/11645 [==============================] - 139s 12ms/step - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 170/200\n",
      "11645/11645 [==============================] - 144s 12ms/step - loss: 0.0048 - accuracy: 0.9990\n",
      "Epoch 171/200\n",
      "11645/11645 [==============================] - 140s 12ms/step - loss: 0.0131 - accuracy: 0.9973\n",
      "Epoch 172/200\n",
      "11645/11645 [==============================] - 138s 12ms/step - loss: 0.0052 - accuracy: 0.9987\n",
      "Epoch 173/200\n",
      "11645/11645 [==============================] - 140s 12ms/step - loss: 0.0058 - accuracy: 0.9987\n",
      "Epoch 174/200\n",
      "11645/11645 [==============================] - 136s 12ms/step - loss: 0.0113 - accuracy: 0.9978\n",
      "Epoch 175/200\n",
      "11645/11645 [==============================] - 134s 11ms/step - loss: 0.0037 - accuracy: 0.9986\n",
      "Epoch 176/200\n",
      "11645/11645 [==============================] - 133s 11ms/step - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 177/200\n",
      "11645/11645 [==============================] - 128s 11ms/step - loss: 0.0053 - accuracy: 0.9984\n",
      "Epoch 178/200\n",
      "11645/11645 [==============================] - 128s 11ms/step - loss: 0.0124 - accuracy: 0.9969\n",
      "Epoch 179/200\n",
      "11645/11645 [==============================] - 134s 11ms/step - loss: 0.0117 - accuracy: 0.9973\n",
      "Epoch 180/200\n",
      "11645/11645 [==============================] - 126s 11ms/step - loss: 0.0047 - accuracy: 0.9986\n",
      "Epoch 181/200\n",
      "11645/11645 [==============================] - 128s 11ms/step - loss: 0.0075 - accuracy: 0.9985\n",
      "Epoch 182/200\n",
      "11645/11645 [==============================] - 126s 11ms/step - loss: 0.0180 - accuracy: 0.9959\n",
      "Epoch 183/200\n",
      "11645/11645 [==============================] - 128s 11ms/step - loss: 0.0116 - accuracy: 0.9969\n",
      "Epoch 184/200\n",
      "11645/11645 [==============================] - 126s 11ms/step - loss: 0.0072 - accuracy: 0.9982\n",
      "Epoch 185/200\n",
      "11645/11645 [==============================] - 126s 11ms/step - loss: 0.0057 - accuracy: 0.9985\n",
      "Epoch 186/200\n",
      "11645/11645 [==============================] - 126s 11ms/step - loss: 0.0083 - accuracy: 0.9979\n",
      "Epoch 187/200\n",
      "11645/11645 [==============================] - 121s 10ms/step - loss: 0.0100 - accuracy: 0.9979\n",
      "Epoch 188/200\n",
      "11645/11645 [==============================] - 129s 11ms/step - loss: 0.0073 - accuracy: 0.9976\n",
      "Epoch 189/200\n",
      "11645/11645 [==============================] - 133s 11ms/step - loss: 0.0148 - accuracy: 0.9975\n",
      "Epoch 190/200\n",
      "11645/11645 [==============================] - 126s 11ms/step - loss: 0.0155 - accuracy: 0.9968\n",
      "Epoch 191/200\n",
      "11645/11645 [==============================] - 126s 11ms/step - loss: 0.0181 - accuracy: 0.9964\n",
      "Epoch 192/200\n",
      "11645/11645 [==============================] - 126s 11ms/step - loss: 0.0158 - accuracy: 0.9967\n",
      "Epoch 193/200\n",
      "11645/11645 [==============================] - 127s 11ms/step - loss: 0.0144 - accuracy: 0.9967\n",
      "Epoch 194/200\n",
      "11645/11645 [==============================] - 127s 11ms/step - loss: 0.0066 - accuracy: 0.9985\n",
      "Epoch 195/200\n",
      "11645/11645 [==============================] - 127s 11ms/step - loss: 0.0049 - accuracy: 0.9989\n",
      "Epoch 196/200\n",
      "11645/11645 [==============================] - 126s 11ms/step - loss: 0.0057 - accuracy: 0.9984\n",
      "Epoch 197/200\n",
      "11645/11645 [==============================] - 120s 10ms/step - loss: 0.0091 - accuracy: 0.9979\n",
      "Epoch 198/200\n",
      "11645/11645 [==============================] - 125s 11ms/step - loss: 0.0021 - accuracy: 0.9989\n",
      "Epoch 199/200\n",
      "11645/11645 [==============================] - 125s 11ms/step - loss: 0.0047 - accuracy: 0.9987\n",
      "Epoch 200/200\n",
      "11645/11645 [==============================] - 125s 11ms/step - loss: 0.0056 - accuracy: 0.9994\n"
     ]
    }
   ],
   "source": [
    "#epoch = 135\n",
    "\n",
    "model = Sequential()\n",
    "print(np.shape(images[0]))\n",
    "model.add(Conv2D(32,(3,3),padding='same',activation='relu',input_shape=np.shape(images[0])))\n",
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',activation='relu'))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',activation='relu'))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(len(labels_encoded[0]),activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 200\n",
    "callbacks = myCallback()   #object of the class created\n",
    "history = model.fit(np.array(images), labels_encoded, epochs=epochs, batch_size= 100 , callbacks=[callbacks]) # 32)\n",
    "\n",
    "model.save('CNN_MODEL.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from *keras.models* , an instance of the Sequential model is created .\n",
    "then using model.add() , convolution filters are added to the model as *Conv2D()* with activation function as *relu*.\n",
    "Therby ,  3 hidden convolution layers and 28 output categorical classes are added to model .\n",
    "\n",
    "*model.summary()* gives the backbone and architecture of the model to be compiled\n",
    "\n",
    "The model is compiled as *model.compile()*  with optimizer as 'adam'and loss as 'categorical_crossentropy'\n",
    "Finally , the model is fitted using *model.fit()* , as per the specified arguments here the epoch is set to as 200 by which the training of the model takes place for 200 epochs and is training is cancelled as per the callback defined before .\n",
    "The model is finally saved as *CNN_MODEL.h5* using *model.save()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 45, 45, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 43, 43, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 21, 21, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 19, 19, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 28)                3612      \n",
      "=================================================================\n",
      "Total params: 216,892\n",
      "Trainable params: 216,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
